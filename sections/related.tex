\section{Related Work}{\label{sec:related}}
Coresets are small summaries of data which 
can be used as a proxy to the original data with provable guarantees. The term was first introduced in \cite{agarwal2004approximating}, where they used coresets for the shape fitting problem. Coresets for clustering problems were described in~\cite{har2004coresets}. 
Feldman et al. \cite{feldman2011unified} gave a generalized framework to construct coresets based on importance sampling using sensitivity scores introduced in \cite{langberg2010universal}. Interested reader can check \cite{woodruff2014sketching, braverman2016new, bachem2017practical}. Various online sampling schemes for spectral approximation are discussed in~\cite{cohen2016online, cohen2017input}. 

Tensor decomposition is unique under minimal assumptions \cite{kruskal1977three}. Therefore it has become very popular in various latent variable modeling applications \cite{anandkumar2014tensor, anandkumar2012method, hsu2012spectral}, learning network parameter of neural networks \cite{janzamin2015beating} etc. However, in general (i.e., without any assumption), most of the tensor problems, including tensor decomposition, are NP-hard \cite{hillar2013most}. There has been much work on fast tensor decomposition techniques. Tensor sketching methods for tensor operations are discussed in \cite{wang2015fast}. They show that by applying FFT to the complete tensor during power iteration, one can save both time and space. This scheme can be used in combination with our scheme. 
A work on element-wise tensor sampling \cite{bhojanapalli2015new} gives a distribution on all the tensor elements and samples a few entries accordingly. For $3$-order, orthogonally decomposable tensors, \cite{song2016sublinear} gives a sub-linear time algorithm for tensor decomposition, which requires the knowledge of norms of slices of the tensor. 
The area of online tensor power iterations has also been explored in \cite{huang2015online, wang2016online}. 
Various heuristics for tensor sketching as well as RandNLA techniques \cite{woodruff2014sketching} over matricized tensors for estimating low-rank tensor approximation have been studied in \cite{song2019relative}. There are few algorithms that use randomized techniques to make CP-ALS, i.e., CP tensor decomposition based on alternating least square method more practical \cite{battaglino2018practical, erichson2020randomized}. Here the author shows various randomized techniques based on sampling and projection to improve the running time and robustness of the CP decomposition. Erichson et al. \cite{erichson2020randomized}, also show that their randomized projection based algorithm can also be used in power iteration based tensor decomposition. For many of these decomposition techniques, our algorithm can be used as a prepossessing. 

In the online setting, for a matrix $\*A \in \~R^{n \times d}$ where rows are coming in streaming manner, the guarantee achieved by \cite{cohen2016online} while preserving additive error spectral approximation $|\|\*A\*x\|^{2} - \|\*C\*x\|^{2}| \leq \epsilon \|\*A\*x\|^{2} + \delta, \forall \*x \in \~R^{d}$, with sample size $O(d(\log d)(\log \epsilon\|\*A\|^{2}/\delta))$.

The problem of $\ell_{p}$ subspace embedding has been explored in both offline~\cite{dasgupta2009sampling, woodruff2013subspace, cohen2015p, clarkson2016fast} and streaming setting~\cite{dickens2018leveraging}. As any offline algorithm to construct coresets can be used as streaming algorithm~\cite{har2004coresets}, we use the known offline algorithms and summarize their results in the streaming version in table \ref{tab:compare}.
Dasgupta et al. \cite{dasgupta2009sampling} show that one can spend $O(nd^{5}\log n)$ time to sample $O(\frac{d^{p+1}}{\epsilon^{2}})$ rows to get a guaranteed $(1\pm \epsilon)$ approximate subspace embedding for any $p$. 
The algorithm in~\cite{woodruff2013subspace} samples $O(n^{1-2/p}\mbox{poly}(d))$ rows and gives $\mbox{poly}(d)$ error relative subspace embedding but in $O(\mbox{nnz}(\*A))$ time. For streaming $\ell_{p}$ subspace embedding~\cite{dickens2018leveraging}, give a one pass deterministic algorithm for $\ell_{p}$ subspace embedding for $1\leq p\leq \infty$. 
For some constant $\gamma \in (0,1)$ the algorithm takes $O(n^{\gamma}d)$ space and $O(n^{\gamma}d^{2}+n^{\gamma}d^{5}\log n)$ update time to return a $1/d^{O(1/\gamma)}$ error relative subspace embedding for any $\ell_{p}$ norm. 
This, however, cannot be made into a constant factor approximation with a sub-linear sample size. We propose various streaming algorithms that give a guaranteed $(1\pm\epsilon)$ relative error approximation for $\ell_{p}$ subspace embedding. 