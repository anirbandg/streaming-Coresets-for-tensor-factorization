\section{Applications}\label{sec:topic}
Here we show how our methods can also be used for learning latent variable models using tensor factorization. We use a corollary, which summarizes the guarantees we get on latent variables by learning them using tensor factorization on our coreset. We discuss it section \ref{sec:application}.
% ~\ref{sec:application}. 
Note that one can always use an even order tensor for estimating the latent variables in a generative model. It will only increase a factor of $O(d)$ in the working space, but by doing so, it will return a smaller coreset, which will be independent of $n$.
\par{\textbf{Streaming Single Topic Model:}}\label{exp}
Here we empirically show how sampling using \online+\kernelfilter~can preserve tensor contraction as in equation \eqref{eq:contract}. This can be used in single topic modeling where documents are coming in a streaming manner. We compare our method with two other sampling schemes, namely -- \uni~and online leverage scores, which we call \online$(2)$.

Here we use a subset of \textbf{20Newsgroups} data (pre-processed). We took a subset of $10$K documents and considered the $100$ most frequent words. We normalized each document vector, such that its $\ell_{1}$ norm is $1$ and created a matrix $\*A \in \~R^{10\text{K} \times 100}$. We feed its row one at a time to \online+\kernelfilter~with $p=3$, which returns a coreset $\*C$. We run tensor based single topic modeling~\cite{anandkumar2014tensor} on $\*A$ and $\*C$, to return $12$ top topic distributions from both. We take the best matching between empirical topics and estimated topics based on $\ell_{1}$ distance and compute the average $\ell_{1}$ difference between them. Here smaller is better. We run this entire method $5$ times and report the median of their $\ell_{1}$ average differences. Here the coreset sizes are over expectation. 

\begin{table}[htbp]
 \caption{Streaming Single Topic Modeling}
 \label{tab:empCompare}
 \vskip 0.1in
 \begin{center}
%   \begin{scriptsize}
   \begin{sc}
    \begin{tabular}{|c|c|c|c|}
     \hline
     Sample & \uni & \online$(2)$ & \online\\
     & & & +\kernelfilter \\
     \hline
     50 & 0.5725 & 0.6903 & \textbf{0.5299}  \\
     \hline
     100 & 0.5093 & 0.6385 & \textbf{0.4379} \\
     \hline
     200 & 0.4687 & 0.5548 & \textbf{0.3231} \\
     \hline
     500 & 0.3777 & 0.3992 & \textbf{0.2173} \\
     \hline
     1000 & 0.2548 & 0.2318 & \textbf{0.1292} \\
     \hline
    \end{tabular} % &
   \end{sc}
%   \end{scriptsize}
 \end{center}
 \vskip -0.1in
\end{table}
% 
From the table, it can be seen that our algorithm \online+\kernelfilter~performs better compare to both \uni~and \online$(2)$, thus supporting our theoretical claims. 
% 
\subsection{Latent Variable Modeling}{\label{sec:application}}
Under the assumption that the data is generated by some generative model such as Gaussian Mixture model, Topic model, Hidden Markov model etc, one can represent the data in terms of higher order (say $3$) moments as $\widetilde{\mcal T_{3}}$ to realize the latent variables~\cite{anandkumar2014tensor}. Next the tensor is reduced to an orthogonally decomposable tensor by multiplying a matrix called whitening matrix ($\*W \in \~R^{d \times k}$), such that $\*W^{T}\*M_{2}\*W = \*I_{k}$. Here $k$ is the number of number of latent variables we are interested and $\*M_{2} \in \~R^{d \times d}$ is the $2^{nd}$ order moment. Now the reduced tensor $\widetilde{\mcal T}_{r} = \widetilde{\mcal T}_{3}(\*W,\*W,\*W)$ is a $k \times k \times k$ orthogonally decomposable tensor. Next by running robust tensor power iteration (RTPI) on $\widetilde{\mcal T}_{r}$ we get the eigenvalue/eigenvector pair on which upon applying inverse whitening transformation we get the estimated latent factors and its corresponding weights \cite{anandkumar2014tensor}.

Note that we give guarantee over the $d \times d \times d$ tensor where as the main theorem 5.3 of~\cite{anandkumar2014tensor} has conditioned over the smaller orthogonally reducible tensor $\widetilde{\mcal T}_{r} \in \~R^{k \times k \times k}$. Now rephrasing the main theorem 5.1 of \cite{anandkumar2014tensor} we get that the $\|\mcal M_{3} - \widetilde{\mcal T}_{3}\| \leq \varepsilon\|\*W\|^{-3}$ where $\mcal M_{3}$ is the true $3$-order tensor with no noise and $\widetilde{\mcal T}_{3}$ is the empirical tensor that we get from the dataset. 
Now we state the guarantees that one gets by applying the RTPI on our sampled data.
\begin{corollary}{\label{coro:tensorFactors}}
 For a dataset $\*A \in \~R^{n \times d}$ with rows coming in streaming fashion and the algorithm \online+\kernelfilter~returns a coreset $\*C$ which guarantees \eqref{eq:contract} such that if for all unit vector $\*x \in \*Q$, it ensures $\epsilon\sum_{i \leq n} |\*a^{T}\*x|^{3} \leq \varepsilon \|\*W\|^{-3}$. Then applying the RTPI on the sampled coreset $\*C$ returns $k$ eigenpairs $\{\lambda_{i},\*v_{i}\}$ of the reduced (orthogonally decomposable) tensor, such that it ensures $\forall i \in [k]$,
 \begin{equation*}
  \|\*v_{\pi(i)} - \*v_{i}\| \leq 8\varepsilon/\lambda_{i} \qquad \qquad \mbox{and} \quad \qquad |\lambda_{\pi(i)}-\lambda_{i}| \leq 5\varepsilon
 \end{equation*}
\end{corollary}
Here precisely we have $\*Q$ as the column space of the $\*W^{\dagger}$, where $\*W$ is a $d \times k$ dimensional whitening matrix as defined above.
% 
\subsubsection{Tensor Contraction}
Now we show empirically that how coreset from \online+\kernelfilter~preserves $4$-order tensor contraction. We compare our method with two other sampling schemes, namely -- \uni~and \online$(2)$. Here \online$(2)$ is the \online~with $p=2$. 

% 
\textbf{Dataset:} We generated a dataset with $200$K rows in $\~R^{30}$. Each coordinate of the row was set with a uniformly generated scalar in $[0,1]$. Further, each row was normalized to have $\ell_{2}$ norm as $1$. So we get a matrix of size $200\text{K} \times 30$, but we ensured that it had rank $12$. Furthermore, $99.99\%$ of the rows in the matrix spanned only an $8$-dimensional subspace in $\~R^{30}$, and it is orthogonal to a $4$ dimensional subspace from which the remaining $0.01\%$ of the rows were generated. We simulated these rows to come in the online fashion and applied the three sampling strategies. From the coreset returned by these sampling strategies, we generated $4$-order tensors $\mcal{\hat{T}}$, and we also create the tensor $\mcal T$ using the entire dataset. The three sampling strategies are \uni, \online$(2)$ and \online+\kernelfilter are as follows.

\textbf{\uni:} Here, we sample rows uniformly at random. It means that every row has a chance of getting sampled with a probability of $r/n$, i.e., every row has an equal probability of getting sampled. Here the parameter $r$ is used to decide the expected number of samples. Intuitively it is highly unlikely to pick a representative row from a subspace spanned by fewer rows. Hence the coreset from this sampling method might not preserve tensor contraction $\forall \*x \in \*Q$.

\textbf{\online$(2)$:} Here, we sample rows based on online leverage scores $c_{i} = \*a_{i}^{T}(\*A_{i}^{T}\*A_{i})^{-1}\*a_{i}$. We define a sampling probability for an incoming row $i$ as $p_{i} = \min\{1,rc_{i}/(\sum_{j=1}^{i}c_{j})\}$. Here the parameter $r$ is used to decide the expected number of samples. Rows with high leverage scores have higher chance of getting sampled. Though leverage score sampling preserved rank of the the data, but it is not known to preserve higher order moments or tensor contractions.

\textbf{\online+\kernelfilter:} Here, every incoming row is first fed to \online. If it samples the row, then it further passed to \kernelfilter, which decides whether to sample the row in the final coreset or not. In the algorithm \kernelfilter~we set the parameter to get desired expected number of samples.

Now we compare the relative error approximation $|\mcal T(\*x,\*x,\*x,\*x) - \hat{\mcal T}(\*x,\*x,\*x,\*x)|/\mcal T(\*x,\*x,\*x,\*x)$, between three sampling schemes mentioned above. Here $\mcal T(\*x,\*x,\*x,\*x) = \sum_{i=1}^{n}(\*a_{i}^T\*x)^4$ and $\hat{\mcal T}(\*x,\*x,\*x,\*x) = \sum_{\tilde{\*a}_i \in \*C}^{n}(\tilde{\*a}_{i}^T\*x)^4$. In table (\ref{tab:query_set}), $\*Q$ is set of right singular vectors of $\*A$ corresponding to the $5$ smallest singular values. This table reports the relative error approximation $|\sum_{\*x\in[\*Q]}\mcal T(\*x,\*x,\*x,\*x) - \sum_{\*x\in[\*Q]}\hat{\mcal T}(\*x,\*x,\*x,\*x)|/\sum_{\*x\in[\*Q]}\mcal T(\*x,\*x,\*x,\*x)$. The table (\ref{tab:max_variance}) reports for $\*x$ as the right singular vector of the smallest singular value of $\*A$. Here we choose this $\*x$ because this direction captures the worst direction, as in the direction which has the highest variance in the sampled data. For each sampling technique and each sample size, we ran $5$ random experiments and reported the median of the experiments. Here, the sample size are in expectation.
% 
\begin{table}[htbp]
 \caption{Relative error for query $\*x \in \*Q$}
 \label{tab:query_set}
 \vskip 0.1in
 \begin{center}
%   \begin{scriptsize}
   \begin{sc}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Sample & \uni & \online$(2)$ & \online\\
     & & & +\kernelfilter \\
    \hline
    200 & 1.1663 & 0.2286 & \textbf{0.1576}  \\
    \hline
    250 & 0.4187 & 0.1169 & \textbf{0.0855} \\
    \hline
    300 & 0.6098 & 0.1195 & \textbf{0.0611} \\
    \hline
    350 & 0.5704 & 0.0470 & \textbf{0.0436} \\
    \hline
    \end{tabular}
   \end{sc}
%   \end{scriptsize}
 \end{center}
 \vskip -0.1in
\end{table}
% 
\begin{table}[htbp]
 \caption{Relative error for query $\*x$ as right singular vector of the smallest singular value}
 \label{tab:max_variance}
 \vskip 0.1in
 \begin{center}
%   \begin{scriptsize}
  \begin{sc}
  \begin{tabular}{|c|c|c|c|}
   \hline
   Sample & \uni & \online$(2)$ & \online\\
     & & & +\kernelfilter \\
   \hline
   100 & 1.3584 & 0.8842 & \textbf{0.6879} \\
   \hline
   200 & 0.8886 & 0.5005 & \textbf{0.3952} \\
   \hline
   300 & 0.8742 & 0.4195 & \textbf{0.3696} \\
   \hline
   500 & 0.9187 & 0.3574 & \textbf{0.2000} \\
   \hline
  \end{tabular}
  \end{sc}
%   \end{scriptsize}
 \end{center}
 \vskip -0.1in
\end{table}

\section{Conclusion}
In this work, we presented both online and streaming algorithms to create coresets for tensor and $\ell_{p}$ subspace embedding, and showed their applications in latent factor models. The algorithms either match or improve upon a number of existing algorithms for $\ell_p$ subspace embedding for all integer $p\ge 2$.  The core of our approach is using a combination of a fast online subroutine \online~for filtering out most rows and a more expensive subroutine for better subspace approximation.  
Obvious open questions include extending the techniques to $p=1$ as well as improving the coreset size for \kernelfilter, for odd-$p$. It will also be interesting to explore the connection of \kernelfilter~to Lewis weights \cite{cohen2015p}, since both are different ways of mapping the $\ell_p$ problem to $\ell_2$. It will also be interesting to explore both theoretically and empirically that how the randomized CP decomposition \cite{battaglino2018practical,erichson2020randomized} performs in various latent variable models.

\paragraph{Acknowledgements.} We are grateful to the anonymous reviewers for their helpful feedback.  Anirban  acknowledges the kind support of the N. Rama Rao Chair Professorship at IIT Gandhinagar, the Google India AI/ML award (2020), Google Faculty Award (2015), and CISCO University Research Grant (2016). Supratim acknowledges the kind support of Additional Fellowship from IIT Gandhinagar.